"""
Cross-experiment comparison: load metrics from multiple experiments and produce
per-house and aggregate comparison DataFrames.

Data sources per experiment (all already generated by the pipeline):
  - evaluation_summaries/dynamic_evaluation_summary_{house}.csv
  - device_sessions/device_sessions_{house}.json
  - house_timing.csv
  - experiment_metadata.json
"""
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)

PHASES = ['w1', 'w2', 'w3']

DEVICE_TYPES = [
    'boiler', 'regular_ac', 'central_ac', 'three_phase_device',
    'recurring_pattern', 'unknown',
]

FEATURE_FLAGS = [
    ('use_nan_imputation', 'nan_imputation'),
    ('use_settling_extension', 'settling_extension'),
    ('use_split_off_merger', 'split_off_merger'),
    ('use_guided_recovery', 'guided_recovery'),
    ('use_wave_recovery', 'wave_recovery'),
]


# ---------------------------------------------------------------------------
# Loading helpers
# ---------------------------------------------------------------------------

def _load_metadata(experiment_dir: Path) -> Optional[dict]:
    """Load experiment_metadata.json."""
    meta_path = experiment_dir / 'experiment_metadata.json'
    if not meta_path.exists():
        return None
    with open(meta_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def _load_timing(experiment_dir: Path) -> Dict[str, float]:
    """Load house_timing.csv → {house_id: elapsed_seconds}."""
    timing_path = experiment_dir / 'house_timing.csv'
    if not timing_path.exists():
        return {}
    df = pd.read_csv(timing_path, dtype={'house_id': str})
    # Take max elapsed per house (handles duplicate rows)
    return (
        df.groupby('house_id')['elapsed_seconds']
        .max()
        .to_dict()
    )


def _load_evaluation_summaries(experiment_dir: Path) -> Dict[str, pd.DataFrame]:
    """Load all dynamic_evaluation_summary CSVs → {house_id: DataFrame}."""
    summaries_dir = experiment_dir / 'evaluation_summaries'
    if not summaries_dir.exists():
        return {}

    result = {}
    for csv_path in summaries_dir.glob('dynamic_evaluation_summary_*.csv'):
        house_id = csv_path.stem.replace('dynamic_evaluation_summary_', '')
        try:
            result[house_id] = pd.read_csv(csv_path)
        except Exception as e:
            logger.warning(f"Failed to load {csv_path}: {e}")
    return result


def _load_device_sessions(experiment_dir: Path) -> Dict[str, dict]:
    """Load all device_sessions JSONs → {house_id: parsed_json}."""
    sessions_dir = experiment_dir / 'device_sessions'
    if not sessions_dir.exists():
        return {}

    result = {}
    for json_path in sessions_dir.glob('device_sessions_*.json'):
        house_id = json_path.stem.replace('device_sessions_', '')
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                result[house_id] = json.load(f)
        except Exception as e:
            logger.warning(f"Failed to load {json_path}: {e}")
    return result


# ---------------------------------------------------------------------------
# Per-experiment loading
# ---------------------------------------------------------------------------

def load_experiment_data(experiment_dir: Path) -> Optional[dict]:
    """Load all comparison-relevant data from one experiment directory.

    Returns dict with keys: experiment, exp_id, description, metadata,
    evaluations, sessions, timing, key_features.
    Returns None if metadata is missing (not a valid experiment dir).
    """
    metadata = _load_metadata(experiment_dir)
    if metadata is None:
        logger.warning(f"No metadata in {experiment_dir}, skipping")
        return None

    exp_config = metadata.get('experiment', {})
    exp_id = exp_config.get('exp_id', experiment_dir.name)
    description = exp_config.get('description', '')

    return {
        'experiment': experiment_dir.name,
        'experiment_dir': experiment_dir,
        'exp_id': exp_id,
        'description': description,
        'metadata': metadata,
        'evaluations': _load_evaluation_summaries(experiment_dir),
        'sessions': _load_device_sessions(experiment_dir),
        'timing': _load_timing(experiment_dir),
        'key_features': _extract_key_features(exp_config),
    }


def _extract_key_features(exp_config: dict) -> List[str]:
    """Extract list of enabled feature names from experiment config."""
    features = []
    for flag, name in FEATURE_FLAGS:
        if exp_config.get(flag, False):
            features.append(name)

    # Normalization
    if exp_config.get('use_normalization', False):
        method = exp_config.get('normalization_method', '?')
        features.append(f'normalization({method})')

    return features


# ---------------------------------------------------------------------------
# Per-house metric extraction
# ---------------------------------------------------------------------------

def extract_house_metrics(exp_data: dict, house_id: str) -> dict:
    """Extract flat metric dict for one house from loaded experiment data.

    Returns a dict matching the per-house CSV columns.
    """
    row = {
        'experiment': exp_data['experiment'],
        'exp_id': exp_data['exp_id'],
        'house_id': house_id,
    }

    # --- Evaluation metrics (M1) ---
    eval_df = exp_data['evaluations'].get(house_id)
    if eval_df is not None and not eval_df.empty:
        last_run = eval_df['run_number'].max()
        last_rows = eval_df[eval_df['run_number'] == last_run]

        total_original = 0.0
        total_explained = 0.0
        for phase in PHASES:
            phase_row = last_rows[last_rows['phase'] == phase]
            if not phase_row.empty:
                pct = phase_row['cumulative_explained_pct'].values[0]
                orig = phase_row['original_power'].values[0]
                row[f'{phase}_explained_pct'] = round(pct, 2)
                total_original += orig
                total_explained += orig * pct / 100.0
            else:
                row[f'{phase}_explained_pct'] = np.nan

        # Weighted average across phases
        if total_original > 0:
            row['avg_explained_pct'] = round(total_explained / total_original * 100, 2)
        else:
            row['avg_explained_pct'] = np.nan
    else:
        for phase in PHASES:
            row[f'{phase}_explained_pct'] = np.nan
        row['avg_explained_pct'] = np.nan

    # --- Identification metrics (M2) ---
    sessions_json = exp_data['sessions'].get(house_id)
    if sessions_json is not None:
        summary = sessions_json.get('summary', {})
        by_type = summary.get('by_device_type', {})

        row['total_sessions'] = summary.get('total_sessions', 0)
        for dt in DEVICE_TYPES:
            row[f'{dt}_count'] = by_type.get(dt, {}).get('count', 0)

        total = row['total_sessions']
        unknown = row.get('unknown_count', 0)
        row['classified_rate'] = round((total - unknown) / total, 3) if total > 0 else 0.0

        # Spike filter stats
        spike_filter = sessions_json.get('spike_filter', {})
        row['spike_count'] = spike_filter.get('spike_count', 0)

        # Average confidence across all sessions
        all_sessions = sessions_json.get('sessions', [])
        confidences = [s.get('confidence', 0) for s in all_sessions if s.get('confidence') is not None]
        row['avg_confidence'] = round(np.mean(confidences), 3) if confidences else np.nan
    else:
        row['total_sessions'] = np.nan
        for dt in DEVICE_TYPES:
            row[f'{dt}_count'] = np.nan
        row['classified_rate'] = np.nan
        row['spike_count'] = np.nan
        row['avg_confidence'] = np.nan

    # --- Timing ---
    row['elapsed_seconds'] = exp_data['timing'].get(house_id, np.nan)

    return row


# ---------------------------------------------------------------------------
# Aggregate metrics
# ---------------------------------------------------------------------------

def compute_aggregate_metrics(exp_data: dict, per_house_rows: List[dict]) -> dict:
    """Compute aggregate statistics across houses for one experiment."""
    df = pd.DataFrame(per_house_rows)
    explained = df['avg_explained_pct'].dropna()

    agg = {
        'experiment': exp_data['experiment'],
        'exp_id': exp_data['exp_id'],
        'description': exp_data['description'],
        'n_houses': len(per_house_rows),
    }

    if len(explained) > 0:
        agg['mean_explained_pct'] = round(explained.mean(), 2)
        agg['median_explained_pct'] = round(explained.median(), 2)
        agg['p25_explained_pct'] = round(explained.quantile(0.25), 2)
        agg['p75_explained_pct'] = round(explained.quantile(0.75), 2)
    else:
        agg['mean_explained_pct'] = np.nan
        agg['median_explained_pct'] = np.nan
        agg['p25_explained_pct'] = np.nan
        agg['p75_explained_pct'] = np.nan

    classified = df['classified_rate'].dropna()
    agg['mean_classified_rate'] = round(classified.mean(), 3) if len(classified) > 0 else np.nan

    for dt in DEVICE_TYPES:
        col = f'{dt}_count'
        agg[f'total_{dt}'] = int(df[col].sum()) if col in df.columns and df[col].notna().any() else 0

    elapsed = df['elapsed_seconds'].dropna()
    agg['mean_elapsed_sec'] = round(elapsed.mean(), 1) if len(elapsed) > 0 else np.nan

    agg['key_features'] = ', '.join(exp_data['key_features'])

    return agg


# ---------------------------------------------------------------------------
# Main comparison
# ---------------------------------------------------------------------------

def compare_experiments(
    experiment_dirs: List[Path],
    common_only: bool = False,
    house_filter: Optional[List[str]] = None,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """Load all experiments, compute per-house and aggregate DataFrames.

    Args:
        experiment_dirs: List of experiment output directories.
        common_only: If True, only include houses present in ALL experiments.
        house_filter: Optional list of house IDs to restrict comparison to.

    Returns:
        (per_house_df, aggregate_df)
    """
    all_per_house = []
    all_aggregate = []

    # Load all experiments
    experiments = []
    for exp_dir in experiment_dirs:
        exp_data = load_experiment_data(Path(exp_dir))
        if exp_data is not None:
            experiments.append(exp_data)

    if not experiments:
        logger.warning("No valid experiments found")
        return pd.DataFrame(), pd.DataFrame()

    # Determine house sets
    if common_only:
        house_sets = [set(exp['evaluations'].keys()) for exp in experiments]
        common_houses = set.intersection(*house_sets) if house_sets else set()
        if house_filter:
            common_houses = common_houses & set(house_filter)
    else:
        common_houses = None  # Use all houses per experiment

    # Process each experiment
    for exp_data in experiments:
        house_ids = sorted(exp_data['evaluations'].keys())
        if common_houses is not None:
            house_ids = [h for h in house_ids if h in common_houses]
        elif house_filter:
            house_ids = [h for h in house_ids if h in house_filter]

        per_house_rows = []
        for house_id in house_ids:
            row = extract_house_metrics(exp_data, house_id)
            per_house_rows.append(row)
            all_per_house.append(row)

        agg = compute_aggregate_metrics(exp_data, per_house_rows)
        all_aggregate.append(agg)

    per_house_df = pd.DataFrame(all_per_house)
    aggregate_df = pd.DataFrame(all_aggregate)

    return per_house_df, aggregate_df


def discover_experiments(base_dir: Path, filter_prefixes: Optional[List[str]] = None) -> List[Path]:
    """Scan a directory for valid experiment output directories.

    A valid experiment directory contains experiment_metadata.json.

    Args:
        base_dir: Directory to scan (e.g., OUTPUT/experiments/).
        filter_prefixes: If given, only include experiments whose directory name
                         starts with one of these prefixes.

    Returns:
        Sorted list of experiment directory paths.
    """
    if not base_dir.exists():
        return []

    dirs = []
    for d in sorted(base_dir.iterdir()):
        if not d.is_dir():
            continue
        if not (d / 'experiment_metadata.json').exists():
            continue
        if filter_prefixes:
            if not any(d.name.startswith(p) for p in filter_prefixes):
                continue
        dirs.append(d)

    return dirs
