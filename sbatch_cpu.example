#!/bin/bash

# Define the directory containing the CSV files
DATA_DIR="/sise/shanigu-group/hilakese-dorins/HouseholdData/"
OUTPUT_DIR="/sise/home/hilakese/jobs"

# Iterate over all CSV files in the directory
for csv_file in "$DATA_DIR"/*.csv; do
    # Extract the base name (house ID without the path and extension)
    house_id=$(basename "$csv_file" .csv)

    # Create a temporary SLURM script for the current house
    temp_script="${OUTPUT_DIR}/job_${house_id}.sbatch"

    cat <<EOT > "$temp_script"
#!/bin/bash
#SBATCH --partition main
#SBATCH --job-name house_${house_id}
#SBATCH --output ${OUTPUT_DIR}/job-${house_id}-%J.out
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=10G
##SBATCH --mail-user=
#SBATCH --mail-type=ALL

### Print some data to output file ###
echo "Processing house: $house_id"
echo "SLURM_JOBID: \$SLURM_JOBID"
echo "SLURM_JOB_NODELIST: \$SLURM_JOB_NODELIST"

### Start your code below ####
module load anaconda				### load anaconda module (must be present when working with conda environments)
source activate nilm_stat_env			### activate a conda environment, replace my_env with your conda environment
python run_scripts.py 0 "$house_id"
EOT

    # Submit the job to SLURM
    sbatch "$temp_script"

    # Optional: Clean up the temporary script after submission
    rm "$temp_script"

done
