{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Device Usage Plot Generator\n",
    "\n",
    "This notebook generates visualizations for specific device activations detected in the experiment analysis.\n",
    "\n",
    "## What This Notebook Does\n",
    "- Parses a list of dates/times from the experiment report\n",
    "- Generates 12-hour window plots for each activation\n",
    "- Shows original power, remaining power, and segmented power for all 3 phases\n",
    "\n",
    "## Where to Get the Data\n",
    "1. Open the **House Report** (house_X.html) from the experiment analysis\n",
    "2. Go to the **Device Detection** section (Central AC, Regular AC, or Boiler)\n",
    "3. Click the **\"Show Copyable Dates\"** button below the table\n",
    "4. Copy the text from the textarea that appears\n",
    "5. Paste it in the `DEVICE_DATES` variable below\n",
    "\n",
    "## Supported Date Formats\n",
    "The notebook supports both formats (can be mixed in the same list):\n",
    "\n",
    "| Format | Example |\n",
    "|--------|---------|\n",
    "| DD/MM/YYYY HH:MM-HH:MM | `10/01/2024 08:30-14:15` |\n",
    "| YYYY-MM-DD HH:MM-HH:MM | `2022-11-11 08:41-09:28` |\n",
    "\n",
    "### Example Input\n",
    "```\n",
    "10/01/2024 08:30-14:15, 2022-11-11 08:41-09:28, 15/01/2024 10:00-16:45\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# Run this cell first when using in Google Colab\n",
    "!pip install pandas plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Colab)\n",
    "# This allows access to files in your Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úì Google Drive mounted successfully\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab - skipping drive mount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "### Set Your Parameters Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - EDIT THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "# House ID (from the report)\n",
    "HOUSE_ID = \"10\"\n",
    "\n",
    "# Path to experiment output directory\n",
    "# For Colab: mount Google Drive and set path like \"/content/drive/MyDrive/experiments/...\"\n",
    "# For local: use relative or absolute path\n",
    "EXPERIMENT_DIR = \"../experiment_pipeline/OUTPUT/experiments/your_experiment_name\"\n",
    "\n",
    "# Run number (usually 0 for first iteration)\n",
    "RUN_NUMBER = 0\n",
    "\n",
    "# ============================================\n",
    "# PASTE DEVICE DATES HERE\n",
    "# Copy from \"Show Copyable Dates\" button in report\n",
    "# ============================================\n",
    "# Supported formats (can be mixed):\n",
    "#   - DD/MM/YYYY HH:MM-HH:MM (e.g., \"10/01/2024 08:30-14:15\")\n",
    "#   - YYYY-MM-DD HH:MM-HH:MM (e.g., \"2022-11-11 08:41-09:28\")\n",
    "DEVICE_DATES = \"\"\"\n",
    "10/01/2024 08:30-14:15, 2022-11-11 08:41-09:28, 15/01/2024 10:00-16:45\n",
    "\"\"\"\n",
    "\n",
    "# Device name (for plot titles)\n",
    "DEVICE_NAME = \"Central AC\"  # Options: \"Central AC\", \"Regular AC\", \"Boiler\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_device_dates(dates_string):\n    \"\"\"\n    Parse device dates from the copied text.\n    \n    Supports two formats:\n    - \"DD/MM/YYYY HH:MM-HH:MM\" (e.g., \"10/01/2024 08:30-14:15\")\n    - \"YYYY-MM-DD HH:MM-HH:MM\" (e.g., \"2022-11-11 08:41-09:28\")\n    \n    Returns list of dicts with:\n        - date: datetime object\n        - on_time: start time string\n        - off_time: end time string  \n        - on_dt: ON datetime\n        - off_dt: OFF datetime\n        - center_time: datetime for plot centering (midpoint of event)\n    \"\"\"\n    activations = []\n    \n    # Split by comma or newline\n    parts = []\n    for line in dates_string.strip().split('\\n'):\n        for p in line.split(','):\n            if p.strip():\n                parts.append(p.strip())\n    \n    for part in parts:\n        date_obj = None\n        on_time = None\n        off_time = None\n        \n        # Try format 1: \"YYYY-MM-DD HH:MM-HH:MM\"\n        match = re.match(r'(\\d{4}-\\d{2}-\\d{2})\\s+(\\d{2}:\\d{2})-(\\d{2}:\\d{2})', part)\n        if match:\n            date_str, on_time, off_time = match.groups()\n            date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n        \n        # Try format 2: \"DD/MM/YYYY HH:MM-HH:MM\"\n        if not date_obj:\n            match = re.match(r'(\\d{2}/\\d{2}/\\d{4})\\s+(\\d{2}:\\d{2})-(\\d{2}:\\d{2})', part)\n            if match:\n                date_str, on_time, off_time = match.groups()\n                date_obj = datetime.strptime(date_str, '%d/%m/%Y')\n        \n        if date_obj and on_time and off_time:\n            on_hour, on_min = map(int, on_time.split(':'))\n            on_dt = date_obj.replace(hour=on_hour, minute=on_min)\n            \n            off_hour, off_min = map(int, off_time.split(':'))\n            off_dt = date_obj.replace(hour=off_hour, minute=off_min)\n            \n            # Handle overnight events\n            if off_dt < on_dt:\n                off_dt += timedelta(days=1)\n            \n            # Calculate midpoint for centering the plot window\n            event_duration = (off_dt - on_dt).total_seconds()\n            center = on_dt + timedelta(seconds=event_duration / 2)\n            \n            label = f\"{date_obj.strftime('%d/%m/%Y')} {on_time}-{off_time}\"\n            \n            activations.append({\n                'date': date_obj,\n                'on_time': on_time,\n                'off_time': off_time,\n                'on_dt': on_dt,\n                'off_dt': off_dt,\n                'center_time': center,\n                'label': label\n            })\n    \n    return activations\n\n\ndef get_required_months(activations, hours_buffer=6):\n    \"\"\"\n    Extract unique months needed for the given activations.\n    Returns set of (month, year) tuples.\n    \n    Args:\n        activations: List of activation dicts from parse_device_dates\n        hours_buffer: Hours before/after event to include (default 6 for 12-hour window)\n    \"\"\"\n    required_months = set()\n    \n    for act in activations:\n        # Get the time range we'll need for this activation\n        start_time = act['center_time'] - timedelta(hours=hours_buffer)\n        end_time = act['center_time'] + timedelta(hours=hours_buffer)\n        \n        # Add all months in this range\n        current = start_time\n        while current <= end_time:\n            required_months.add((current.month, current.year))\n            # Move to next month (always use day=1 to avoid \"day out of range\" errors)\n            if current.month == 12:\n                current = current.replace(year=current.year + 1, month=1, day=1)\n            else:\n                current = current.replace(month=current.month + 1, day=1)\n    \n    return required_months\n\n\ndef load_matches_for_month(experiment_dir, house_id, run_number, month, year):\n    \"\"\"\n    Load both matched events and unmatched individual events for a specific month.\n    \n    Returns DataFrame with all events (matched + unmatched) or None if not found.\n    \"\"\"\n    import pickle\n    import sys\n    import numpy as np\n    \n    # Fix numpy compatibility\n    if not hasattr(sys.modules.get('numpy', None), '_core'):\n        sys.modules['numpy._core'] = np.core\n        sys.modules['numpy._core.numeric'] = np.core.numeric\n    \n    exp_path = Path(experiment_dir)\n    events_list = []\n    \n    # Load matches\n    matches_path = exp_path / f\"run_{run_number}\" / f\"house_{house_id}\" / \"matches\"\n    if matches_path.exists():\n        match_file = matches_path / f\"matches_{house_id}_{month:02d}_{year}.pkl\"\n        if match_file.exists():\n            try:\n                with open(match_file, 'rb') as f:\n                    matches_df = pickle.load(f)\n                    events_list.append(matches_df)\n            except Exception as e:\n                print(f\"Warning: Could not load {match_file}: {e}\")\n    \n    # Load unmatched events from on_off\n    on_off_path = exp_path / f\"run_{run_number}\" / f\"house_{house_id}\" / \"on_off\"\n    if on_off_path.exists():\n        on_off_file = on_off_path / f\"on_off_{house_id}_{month:02d}_{year}.pkl\"\n        if on_off_file.exists():\n            try:\n                with open(on_off_file, 'rb') as f:\n                    on_off_df = pickle.load(f)\n                    # Only include unmatched events\n                    unmatched = on_off_df[on_off_df['matched'] == 0].copy()\n                    if not unmatched.empty:\n                        events_list.append(unmatched)\n            except Exception as e:\n                print(f\"Warning: Could not load {on_off_file}: {e}\")\n    \n    if events_list:\n        combined = pd.concat(events_list, ignore_index=True)\n        # Convert datetime columns\n        for col in ['on_start', 'on_end', 'off_start', 'off_end', 'start', 'end']:\n            if col in combined.columns:\n                combined[col] = pd.to_datetime(combined[col], errors='coerce')\n        return combined\n    \n    return None\n\n\ndef load_summarized_data(experiment_dir, house_id, run_number=0, required_months=None):\n    \"\"\"\n    Load summarized data from monthly pickle files in summarized directory.\n    Only loads months specified in required_months for efficiency.\n    \n    Args:\n        experiment_dir: Path to experiment directory\n        house_id: House ID\n        run_number: Run number (default 0)\n        required_months: Set of (month, year) tuples to load. If None, loads all.\n    \n    Returns:\n        DataFrame with all monthly data concatenated\n    \"\"\"\n    import pickle\n    import sys\n    import numpy as np\n    \n    # Fix numpy compatibility for pickle files created with newer numpy\n    if not hasattr(sys.modules.get('numpy', None), '_core'):\n        sys.modules['numpy._core'] = np.core\n        sys.modules['numpy._core.numeric'] = np.core.numeric\n    \n    exp_path = Path(experiment_dir)\n    \n    # Try new structure: experiment/run_N/house_{id}/summarized/\n    new_path = exp_path / f\"run_{run_number}\" / f\"house_{house_id}\" / \"summarized\"\n    print(f\"Trying path: {new_path}\")\n    \n    if new_path.exists():\n        # Find all pickle files (format: summarized_HOUSEID_MM_YYYY.pkl)\n        all_pickle_files = sorted([f for f in os.listdir(new_path) \n                              if f.endswith('.pkl') and f.startswith(f'summarized_{house_id}_')])\n        \n        # Filter to only required months if specified\n        if required_months:\n            pickle_files = []\n            for pkl_file in all_pickle_files:\n                # Parse month and year from filename: summarized_1001_02_2021.pkl\n                parts = pkl_file.replace('.pkl', '').split('_')\n                if len(parts) >= 4:\n                    try:\n                        month = int(parts[-2])\n                        year = int(parts[-1])\n                        if (month, year) in required_months:\n                            pickle_files.append(pkl_file)\n                    except ValueError:\n                        continue\n            print(f\"Loading {len(pickle_files)} out of {len(all_pickle_files)} months (only required months)\")\n        else:\n            pickle_files = all_pickle_files\n            print(f\"Loading all {len(pickle_files)} months\")\n        \n        if pickle_files:\n            dfs = []\n            \n            for pkl_file in pickle_files:\n                file_path = new_path / pkl_file\n                print(f\"  Loading {pkl_file}...\")\n                try:\n                    with open(file_path, 'rb') as f:\n                        df_month = pickle.load(f)\n                        dfs.append(df_month)\n                except Exception as e:\n                    print(f\"  Warning: Could not load {pkl_file}: {e}\")\n                    continue\n            \n            if dfs:\n                df = pd.concat(dfs, ignore_index=True)\n                df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', dayfirst=True)\n                df = df.sort_values('timestamp').reset_index(drop=True)\n                print(f\"‚úì Loaded total of {len(df)} rows from {len(dfs)} months\")\n                return df\n    \n    # Try old structure: experiment/house_{id}/run_N/house_{id}/summarized/\n    old_path = exp_path / f\"house_{house_id}\" / f\"run_{run_number}\" / f\"house_{house_id}\" / \"summarized\"\n    print(f\"Trying old structure: {old_path}\")\n    \n    if old_path.exists():\n        all_pickle_files = sorted([f for f in os.listdir(old_path) \n                              if f.endswith('.pkl') and f.startswith(f'summarized_{house_id}_')])\n        \n        # Filter to only required months if specified\n        if required_months:\n            pickle_files = []\n            for pkl_file in all_pickle_files:\n                parts = pkl_file.replace('.pkl', '').split('_')\n                if len(parts) >= 4:\n                    try:\n                        month = int(parts[-2])\n                        year = int(parts[-1])\n                        if (month, year) in required_months:\n                            pickle_files.append(pkl_file)\n                    except ValueError:\n                        continue\n            print(f\"Loading {len(pickle_files)} out of {len(all_pickle_files)} months (only required months)\")\n        else:\n            pickle_files = all_pickle_files\n            print(f\"Loading all {len(pickle_files)} months\")\n        \n        if pickle_files:\n            dfs = []\n            \n            for pkl_file in pickle_files:\n                file_path = old_path / pkl_file\n                print(f\"  Loading {pkl_file}...\")\n                try:\n                    with open(file_path, 'rb') as f:\n                        df_month = pickle.load(f)\n                        dfs.append(df_month)\n                except Exception as e:\n                    print(f\"  Warning: Could not load {pkl_file}: {e}\")\n                    continue\n            \n            if dfs:\n                df = pd.concat(dfs, ignore_index=True)\n                df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', dayfirst=True)\n                df = df.sort_values('timestamp').reset_index(drop=True)\n                print(f\"‚úì Loaded total of {len(df)} rows from {len(dfs)} months\")\n                return df\n    \n    raise FileNotFoundError(\n        f\"Could not find summarized directory with pickle files for house {house_id}\\n\"\n        f\"Tried:\\n  - {new_path}\\n  - {old_path}\"\n    )\n\n\ndef filter_data_by_window(df, center_time, hours_before=6, hours_after=6):\n    \"\"\"\n    Filter data to a window around the center time.\n    Center time should be the midpoint of the event.\n    \"\"\"\n    start = center_time - timedelta(hours=hours_before)\n    end = center_time + timedelta(hours=hours_after)\n    \n    print(f\"Filtering data: {start} to {end}\")\n    filtered = df[(df['timestamp'] >= start) & (df['timestamp'] <= end)].copy()\n    \n    if filtered.empty:\n        raise ValueError(f\"No data found for window {start} to {end}\")\n    \n    print(f\"Found {len(filtered)} rows in time window\")\n    return filtered"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def calculate_y_axis_range(df, phases):\n    \"\"\"Calculate shared y-axis range across all phases.\"\"\"\n    y_min = 0  # Start from 0 for power data\n    y_max = 0\n\n    for phase in phases:\n        # Check original columns for the max range\n        columns = [f'original_{phase}']\n        for col in columns:\n            if col in df.columns:\n                col_max = df[col].max()\n                if pd.notna(col_max):\n                    y_max = max(y_max, col_max)\n\n    # Add some padding to see everything\n    if y_max > 0:\n        y_max = y_max * 1.1  # Add 10% padding at top\n    else:\n        y_max = 100  # Default if no data\n    \n    print(f\"Y-axis range calculated: {y_min} to {y_max}\")\n    return [y_min, y_max]\n\n\ndef create_device_plot(df, activation_info, device_name, house_id, matches_df=None):\n    \"\"\"\n    Create a 4-row x 3-column plot showing power data for all phases.\n    Event window (ON-OFF) is shown as shaded background in all rows.\n    \n    Rows:\n    1. Original power\n    2. Remaining power (after segregation)\n    3. Segregated power (short, medium, long duration)\n    4. Events/Matches detected (matched events as connecting lines, unmatched as individual markers)\n    \n    Columns: w1, w2, w3 phases\n    \"\"\"\n    phases = ['w1', 'w2', 'w3']\n    \n    # Calculate shared Y-axis range from original power data\n    y_range = calculate_y_axis_range(df, phases)\n    \n    # Color scheme matching pipeline visualization\n    COLORS = {\n        'original': 'black',\n        'remaining': 'blue',\n        'short': 'red',\n        'medium': 'orange',\n        'long': 'purple',\n        'matched': 'green',\n        'unmatched_on': 'red',\n        'unmatched_off': 'blue',\n    }\n    \n    fig = make_subplots(\n        rows=4, cols=len(phases),\n        shared_xaxes=True, shared_yaxes=False,\n        subplot_titles=[f\"Phase {phase}\" for phase in phases],\n        vertical_spacing=0.06,\n        horizontal_spacing=0.05,\n        row_heights=[0.25, 0.25, 0.25, 0.25]\n    )\n    \n    # Get ON/OFF times for shaded background\n    on_dt = activation_info['on_dt']\n    off_dt = activation_info['off_dt']\n    \n    print(f\"Event window: {on_dt} to {off_dt}\")\n    \n    # Add data traces and shaded backgrounds for each phase\n    for col_idx, phase in enumerate(phases, start=1):\n        # Add shaded background for event window in all 4 rows\n        for row_idx in range(1, 5):\n            # Calculate correct yref for plotly subplots\n            axis_num = (row_idx - 1) * 3 + col_idx\n            yref = 'y domain' if axis_num == 1 else f'y{axis_num} domain'\n            \n            fig.add_shape(\n                type=\"rect\",\n                x0=on_dt, x1=off_dt,\n                y0=0, y1=1,\n                yref=yref,\n                fillcolor=\"rgba(255, 200, 200, 0.3)\",\n                layer=\"below\",\n                line_width=0,\n                row=row_idx, col=col_idx\n            )\n        \n        # Row 1: Original data\n        original_col = f'original_{phase}'\n        if original_col in df.columns:\n            fig.add_trace(\n                go.Scatter(\n                    x=df['timestamp'], y=df[original_col],\n                    mode='lines', line=dict(color=COLORS['original'], width=1),\n                    name=f'Original' if col_idx == 1 else None,\n                    showlegend=(col_idx == 1),\n                    legendgroup='original'\n                ),\n                row=1, col=col_idx\n            )\n        \n        # Row 2: After segregation (remaining)\n        remaining_col = f'remaining_{phase}'\n        if remaining_col in df.columns:\n            fig.add_trace(\n                go.Scatter(\n                    x=df['timestamp'], y=df[remaining_col],\n                    mode='lines', line=dict(color=COLORS['remaining'], width=1),\n                    name=f'Remaining' if col_idx == 1 else None,\n                    showlegend=(col_idx == 1),\n                    legendgroup='remaining'\n                ),\n                row=2, col=col_idx\n            )\n        \n        # Row 3: Segregated by duration (if columns exist)\n        traces_added = 0\n        for duration_type in ['short', 'medium', 'long']:\n            col_name = f'{duration_type}_duration_{phase}'\n            if col_name in df.columns:\n                y_data = df[col_name].fillna(0)\n                if y_data.sum() > 0:  # Only plot if there's actual data\n                    fig.add_trace(\n                        go.Scatter(\n                            x=df['timestamp'], y=y_data,\n                            mode='lines', line=dict(color=COLORS[duration_type], width=1),\n                            name=f'{duration_type.capitalize()} duration' if col_idx == 1 else None,\n                            showlegend=(col_idx == 1),\n                            legendgroup=duration_type\n                        ),\n                        row=3, col=col_idx\n                    )\n                    traces_added += 1\n        \n        if col_idx == 1 and traces_added == 0:\n            print(f\"‚ö† Warning: No traces added to Row 3 (Segregation Data)\")\n        \n        # Row 4: Events - both matched (connecting lines) and unmatched (individual markers)\n        events_added = 0\n        matched_count = 0\n        unmatched_count = 0\n        \n        if matches_df is not None and not matches_df.empty:\n            # Filter events to the current window AND phase\n            window_start = df['timestamp'].min()\n            window_end = df['timestamp'].max()\n\n            # Check for phase column\n            if 'phase' in matches_df.columns:\n                matches_df_copy = matches_df.copy()\n\n                # Convert datetime columns\n                for col in ['on_start', 'on_end', 'off_start', 'off_end', 'start', 'end']:\n                    if col in matches_df_copy.columns:\n                        matches_df_copy[col] = pd.to_datetime(matches_df_copy[col], errors='coerce')\n\n                # Filter to events for THIS PHASE\n                phase_events = matches_df_copy[matches_df_copy['phase'] == phase].copy()\n                \n                # Filter by time window - handle NaN values properly\n                if 'on_start' in phase_events.columns and 'start' in phase_events.columns:\n                    # For matched events: check on_start is in window\n                    # For unmatched events: check start is in window\n                    mask = (\n                        ((pd.notna(phase_events['on_start'])) & \n                         (phase_events['on_start'] >= window_start) & \n                         (phase_events['on_start'] <= window_end)) |\n                        ((pd.notna(phase_events['start'])) & \n                         (phase_events['start'] >= window_start) & \n                         (phase_events['start'] <= window_end))\n                    )\n                    phase_events = phase_events[mask]\n                elif 'on_start' in phase_events.columns:\n                    phase_events = phase_events[\n                        (pd.notna(phase_events['on_start'])) &\n                        (phase_events['on_start'] >= window_start) & \n                        (phase_events['on_start'] <= window_end)\n                    ]\n                elif 'start' in phase_events.columns:\n                    phase_events = phase_events[\n                        (pd.notna(phase_events['start'])) &\n                        (phase_events['start'] >= window_start) & \n                        (phase_events['start'] <= window_end)\n                    ]\n\n                print(f\"Events in window for {phase}: {len(phase_events)}\")\n\n                # Draw each event\n                for idx, event in phase_events.iterrows():\n                    # Check if this is a matched event (has both on_start and off_start with non-null values)\n                    if pd.notna(event.get('on_start')) and pd.notna(event.get('off_start')):\n                        # This is a matched event - draw connecting line\n                        on_mag = event.get('on_magnitude', 0)\n                        off_mag = event.get('off_magnitude', 0)\n                        \n                        fig.add_trace(\n                            go.Scatter(\n                                x=[event['on_start'], event['off_start']],\n                                y=[on_mag, off_mag],\n                                mode='lines+markers',\n                                line=dict(color='green', width=2, dash='solid'),\n                                marker=dict(size=8, color=['green', 'red']),\n                                showlegend=False,\n                                hovertemplate=(\n                                    f\"<b>Match</b><br>\"\n                                    f\"ON:  {event['on_start']}<br>\"\n                                    f\"OFF: {event['off_start']}<br>\"\n                                    f\"Duration: {event.get('duration', 'N/A'):.0f} min<br>\"\n                                    f\"ON mag:  {on_mag:.0f}W<br>\"\n                                    f\"OFF mag: {off_mag:.0f}W<br>\"\n                                    f\"<extra></extra>\"\n                                )\n                            ),\n                            row=4, col=col_idx\n                        )\n                        events_added += 1\n                        matched_count += 1\n                    elif pd.notna(event.get('start')) and pd.notna(event.get('magnitude')):\n                        # This is an unmatched individual event\n                        event_type = event.get('event', 'unknown')\n                        color = COLORS['unmatched_on'] if event_type == 'on' else COLORS['unmatched_off']\n                        \n                        fig.add_trace(\n                            go.Scatter(\n                                x=[event['start'], event['end']],\n                                y=[0, event['magnitude']],\n                                mode='lines+markers',\n                                line=dict(dash='dash', color=color, width=1),\n                                marker=dict(size=6),\n                                showlegend=False,\n                                hovertemplate=(\n                                    f\"<b>Unmatched {event_type.upper()}</b><br>\"\n                                    f\"Start: {event['start']}<br>\"\n                                    f\"End: {event['end']}<br>\"\n                                    f\"Magnitude: {event['magnitude']:.0f}W<br>\"\n                                    f\"<extra></extra>\"\n                                )\n                            ),\n                            row=4, col=col_idx\n                        )\n                        events_added += 1\n                        unmatched_count += 1\n            else:\n                if col_idx == 1:\n                    print(f\"‚ö† Events df missing phase column\")\n\n        if col_idx == 1:\n            print(f\"  Added {events_added} events to Row 4 ({matched_count} matched, {unmatched_count} unmatched)\")\n    \n    # Row titles on y-axis\n    row_titles = [\"Original Data\", \"After Segregation\", \"Segregation Data\", \"Events\"]\n    for row_idx, title in enumerate(row_titles, start=1):\n        fig.update_yaxes(title_text=title, row=row_idx, col=1)\n    \n    # Apply shared Y-axis range to all rows and columns\n    for row_idx in range(1, 5):\n        for col_idx in range(1, 4):\n            fig.update_yaxes(range=y_range, row=row_idx, col=col_idx)\n    \n    # Update layout\n    fig.update_layout(\n        title=f\"{device_name} - House {house_id}<br><sub>{activation_info['label']}</sub>\",\n        hovermode=\"x unified\",\n        showlegend=True,\n        legend=dict(\n            orientation=\"h\",\n            yanchor=\"bottom\",\n            y=1.02,\n            xanchor=\"right\",\n            x=1\n        ),\n        height=900\n    )\n    \n    return fig"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Data and Parse Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the device dates\n",
    "activations = parse_device_dates(DEVICE_DATES)\n",
    "\n",
    "print(f\"Found {len(activations)} activations:\")\n",
    "for i, act in enumerate(activations, 1):\n",
    "    print(f\"  {i}. {act['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the house data - only for required months\n",
    "try:\n",
    "    # First, get the required months from the activations\n",
    "    required_months = get_required_months(activations)\n",
    "    print(f\"\\nRequired months: {sorted(required_months)}\")\n",
    "    print(f\"Total: {len(required_months)} months\\n\")\n",
    "    \n",
    "    # Load only the required months\n",
    "    df = load_summarized_data(EXPERIMENT_DIR, HOUSE_ID, RUN_NUMBER, required_months=required_months)\n",
    "    print(f\"\\nLoaded data for house {HOUSE_ID}\")\n",
    "    print(f\"Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"Total rows: {len(df)}\")\n",
    "    print(f\"\\nAvailable columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Check for segregation columns\n",
    "    seg_cols = [col for col in df.columns if 'duration' in col]\n",
    "    if seg_cols:\n",
    "        print(f\"‚úì Found segregation columns: {seg_cols}\")\n",
    "    else:\n",
    "        print(\"‚ö† No segregation columns found (short/medium/long_duration)\")\n",
    "    \n",
    "    # Load matches for the required months\n",
    "    print(\"\\n--- Loading matches/events ---\")\n",
    "    matches_dfs = []\n",
    "    for month, year in required_months:\n",
    "        matches_df = load_matches_for_month(EXPERIMENT_DIR, HOUSE_ID, RUN_NUMBER, month, year)\n",
    "        if matches_df is not None:\n",
    "            print(f\"‚úì Loaded matches for {month}/{year}: {len(matches_df)} events\")\n",
    "            matches_dfs.append(matches_df)\n",
    "        else:\n",
    "            print(f\"  No matches found for {month}/{year}\")\n",
    "    \n",
    "    if matches_dfs:\n",
    "        all_matches = pd.concat(matches_dfs, ignore_index=True)\n",
    "        print(f\"\\n‚úì Total matches loaded: {len(all_matches)}\")\n",
    "        print(f\"Matches columns: {list(all_matches.columns)}\")\n",
    "    else:\n",
    "        all_matches = None\n",
    "        print(\"\\n‚ö† No matches data found\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. EXPERIMENT_DIR path is correct\")\n",
    "    print(\"2. HOUSE_ID exists in the experiment\")\n",
    "    print(\"3. RUN_NUMBER is valid\")\n",
    "    df = None\n",
    "    all_matches = None"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Check segregation output: verify summarized files have actual segregated data\nimport pickle, sys\nimport numpy as np\n\nif not hasattr(sys.modules.get('numpy', None), '_core'):\n    sys.modules['numpy._core'] = np.core\n    sys.modules['numpy._core.numeric'] = np.core.numeric\n\nexp_path = Path(EXPERIMENT_DIR)\nsummarized_dir = exp_path / f\"run_{RUN_NUMBER}\" / f\"house_{HOUSE_ID}\" / \"summarized\"\n\nif summarized_dir.exists():\n    phases = ['w1', 'w2', 'w3']\n    dur_types = ['short', 'medium', 'long']\n    months_ok, months_empty, total_kwh = 0, 0, 0.0\n\n    for f in sorted(summarized_dir.glob(f\"summarized_{HOUSE_ID}_*.pkl\")):\n        with open(f, 'rb') as fh:\n            month_df = pickle.load(fh)\n        s = sum(month_df[c].fillna(0).sum() for c in month_df.columns if 'duration' in c)\n        if s > 0:\n            months_ok += 1\n            total_kwh += s / 60000\n        else:\n            months_empty += 1\n            print(f\"  EMPTY: {f.stem}\")\n\n    print(f\"\\nHouse {HOUSE_ID}: {months_ok} months with data, {months_empty} empty, {total_kwh:.1f} kWh total\")\n    if months_empty == 0:\n        print(\"All months have segregated data\")\n    else:\n        print(f\"WARNING: {months_empty} months have no segregated data!\")\nelse:\n    print(f\"Summarized directory not found: {summarized_dir}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate plots for all activations\nif df is not None:\n    for i, act in enumerate(activations, 1):\n        print(f\"\\n{'='*60}\")\n        print(f\"Generating plot {i}/{len(activations)}: {act['label']}\")\n        print(f\"{'='*60}\")\n        \n        try:\n            filtered_df = filter_data_by_window(df, act['center_time'])\n            \n            # Debug: show segregation data sums per phase in this window\n            phases = ['w1', 'w2', 'w3']\n            print(f\"\\n  Segregation data in filtered window ({len(filtered_df)} rows):\")\n            for phase in phases:\n                phase_sums = []\n                for dur in ['short', 'medium', 'long']:\n                    col = f'{dur}_duration_{phase}'\n                    if col in filtered_df.columns:\n                        s = filtered_df[col].fillna(0).sum()\n                        phase_sums.append(f\"{dur}={s:.0f}\")\n                    else:\n                        phase_sums.append(f\"{dur}=NO COL\")\n                total = sum(filtered_df[f'{d}_duration_{phase}'].fillna(0).sum() \n                           for d in ['short', 'medium', 'long'] \n                           if f'{d}_duration_{phase}' in filtered_df.columns)\n                print(f\"    {phase}: {', '.join(phase_sums)}  (total={total:.0f}W)\")\n            \n            fig = create_device_plot(filtered_df, act, DEVICE_NAME, HOUSE_ID, matches_df=all_matches)\n            fig.show()\n        except ValueError as e:\n            print(f\"  Skipping - {e}\")\nelse:\n    print(\"No data loaded. Please fix the errors above.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = f\"./plots/{DEVICE_NAME.replace(' ', '_').lower()}_{HOUSE_ID}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Install kaleido for image export (only needed once)\n",
    "try:\n",
    "    import kaleido\n",
    "except ImportError:\n",
    "    print(\"Installing kaleido for image export...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'kaleido'])\n",
    "    import kaleido\n",
    "\n",
    "if df is not None:\n",
    "    # Save all plots as HTML and PNG files\n",
    "    for i, act in enumerate(activations, 1):\n",
    "        try:\n",
    "            filtered_df = filter_data_by_window(df, act['center_time'])\n",
    "            fig = create_device_plot(filtered_df, act, DEVICE_NAME, HOUSE_ID, matches_df=all_matches)\n",
    "            \n",
    "            # Create filename from date\n",
    "            date_str = act['date'].strftime('%Y%m%d')\n",
    "            time_str = act['on_time'].replace(':', '')\n",
    "            base_filename = f\"{OUTPUT_DIR}/plot_{date_str}_{time_str}\"\n",
    "            \n",
    "            # Save as HTML (interactive, download to view)\n",
    "            html_file = f\"{base_filename}.html\"\n",
    "            fig.write_html(html_file)\n",
    "            print(f\"‚úì HTML: {html_file}\")\n",
    "            \n",
    "            # Save as PNG (static image, view directly in Drive)\n",
    "            png_file = f\"{base_filename}.png\"\n",
    "            fig.write_image(png_file, width=1400, height=900)\n",
    "            print(f\"‚úì PNG:  {png_file}\")\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"Skipped {act['label']}: {e}\")\n",
    "\n",
    "    print(f\"\\nüìÅ All plots saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"  ‚Ä¢ HTML files - Download to view interactive plots\")\n",
    "    print(f\"  ‚Ä¢ PNG files  - View directly in Google Drive\")\n",
    "else:\n",
    "    print(\"No data loaded. Cannot save plots.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}